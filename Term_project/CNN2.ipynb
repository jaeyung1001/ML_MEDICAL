{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import _pickle as cPickle\n",
    "from collections import defaultdict\n",
    "import torch\n",
    "import torch.nn.init\n",
    "from torch.autograd import Variable\n",
    "import tensorflow as tf\n",
    "from tensorflow.examples.tutorials.mnist import input_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dirname = './data_preprocessed_python'\n",
    "filenames = os.listdir(dirname)\n",
    "data_dic = defaultdict(lambda:[])\n",
    "for i, filename in enumerate(filenames):\n",
    "    full_filename = os.path.join(dirname, filename)\n",
    "    x = cPickle.load(open(full_filename, 'rb'), encoding='ISO-8859-1')\n",
    "    data_dic[filename[:-4]] = x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from scipy.stats import kurtosis, skew\n",
    "def preprocessing(data):\n",
    "    result = []\n",
    "    mean = np.mean(data)\n",
    "    median = np.median(data)\n",
    "    maximum = np.amax(data)\n",
    "    minimum = np.amin(data)\n",
    "    std_dev = np.std(data)\n",
    "    var = np.var(data)\n",
    "    ran = np.ptp(data)\n",
    "    skewness = skew(data)\n",
    "    kurto = kurtosis(data)\n",
    "    result=[mean, median, maximum, minimum,\n",
    "                   std_dev, var, ran, skewness, kurto]\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "new_data_dic = defaultdict(lambda:defaultdict(lambda:[]))\n",
    "for key in data_dic.keys():\n",
    "    par = int(key[1:])\n",
    "    for i, vid in enumerate(data_dic[key]['data']):\n",
    "        new_data_dic[key]['data'].append([])\n",
    "        for chnn in vid[:-8]:\n",
    "            l =preprocessing(np.array(chnn))\n",
    "            for batch in range(0,10):\n",
    "                l = l+preprocessing(np.array(chnn[batch*807:max((batch+1)*807, 8064)]))\n",
    "            new_data_dic[key]['data'][i].append(np.array([par, i+1]+l))\n",
    "    for i,  vid in enumerate(data_dic[key]['labels']):\n",
    "        l2 = []\n",
    "        l3 = []\n",
    "        new_data_dic[key]['labels'].append([])\n",
    "        new_data_dic[key]['labels2'].append([])\n",
    "        for value in vid:\n",
    "            if value>=5:\n",
    "                l2.append(1)\n",
    "                l2.append(0)\n",
    "            else:\n",
    "                l2.append(0)\n",
    "                l2.append(1)\n",
    "            if value>=6:\n",
    "                l3.append(1)\n",
    "                l3.append(0)\n",
    "                l3.append(0)\n",
    "            elif value>=4 and value<6:\n",
    "                l3.append(0)\n",
    "                l3.append(1)\n",
    "                l3.append(0)\n",
    "            else:\n",
    "                l3.append(0)\n",
    "                l3.append(0)\n",
    "                l3.append(1)\n",
    "        new_data_dic[key]['labels'][i] += l2\n",
    "        new_data_dic[key]['labels2'][i] += l3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32\n"
     ]
    }
   ],
   "source": [
    "print(len(new_data_dic['s01']['data'][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32\n",
      "1240\n",
      "(array([[  1.00000000e+00,   2.00000000e+00,  -8.27121269e-02, ...,\n",
      "          3.70042683e+01,  -5.08154218e-02,   8.69235014e-01],\n",
      "       [  1.00000000e+00,   2.00000000e+00,  -3.13505384e-02, ...,\n",
      "          4.18200988e+01,   1.11964916e-01,   1.39442973e+00],\n",
      "       [  1.00000000e+00,   2.00000000e+00,  -2.04664705e-02, ...,\n",
      "          3.87172925e+01,   5.88444745e-02,   9.21659624e-01],\n",
      "       ..., \n",
      "       [  1.00000000e+00,   2.00000000e+00,   6.82457494e-02, ...,\n",
      "          2.08783110e+01,   1.04035679e-01,   1.35067118e-03],\n",
      "       [  1.00000000e+00,   2.00000000e+00,   1.42352938e-01, ...,\n",
      "          5.25603937e+01,   1.71642208e-01,   2.84520300e+00],\n",
      "       [  1.00000000e+00,   2.00000000e+00,   1.56159050e-01, ...,\n",
      "          3.58672933e+01,   3.79596399e-01,   9.76703888e-01]]), [1, 0, 1, 0, 1, 0, 1, 0])\n"
     ]
    }
   ],
   "source": [
    "data_loader = []\n",
    "test_loader = []\n",
    "idx = 0\n",
    "print(len(new_data_dic.keys()))\n",
    "for par in new_data_dic.keys():\n",
    "    if par != 's02':\n",
    "        for i, vid in enumerate(new_data_dic[par]['data']):\n",
    "            data_loader.append((np.array(vid), new_data_dic[par]['labels'][i]))\n",
    "            idx +=1\n",
    "    else:\n",
    "        for i, vid in enumerate(new_data_dic[par]['data']):\n",
    "            test_loader.append((np.array(vid), new_data_dic[par]['labels'][i]))\n",
    "            idx +=1\n",
    "print(len(data_loader))\n",
    "print(data_loader[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32\n",
      "1240\n",
      "(array([[  1.00000000e+00,   2.00000000e+00,  -8.27121269e-02, ...,\n",
      "          3.70042683e+01,  -5.08154218e-02,   8.69235014e-01],\n",
      "       [  1.00000000e+00,   2.00000000e+00,  -3.13505384e-02, ...,\n",
      "          4.18200988e+01,   1.11964916e-01,   1.39442973e+00],\n",
      "       [  1.00000000e+00,   2.00000000e+00,  -2.04664705e-02, ...,\n",
      "          3.87172925e+01,   5.88444745e-02,   9.21659624e-01],\n",
      "       ..., \n",
      "       [  1.00000000e+00,   2.00000000e+00,   6.82457494e-02, ...,\n",
      "          2.08783110e+01,   1.04035679e-01,   1.35067118e-03],\n",
      "       [  1.00000000e+00,   2.00000000e+00,   1.42352938e-01, ...,\n",
      "          5.25603937e+01,   1.71642208e-01,   2.84520300e+00],\n",
      "       [  1.00000000e+00,   2.00000000e+00,   1.56159050e-01, ...,\n",
      "          3.58672933e+01,   3.79596399e-01,   9.76703888e-01]]), [1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0])\n"
     ]
    }
   ],
   "source": [
    "data_loader2 = []\n",
    "test_loader2 = []\n",
    "idx = 0\n",
    "print(len(new_data_dic.keys()))\n",
    "for par in new_data_dic.keys():\n",
    "    if par != 's02':\n",
    "        for i, vid in enumerate(new_data_dic[par]['data']):\n",
    "            data_loader2.append((np.array(vid), new_data_dic[par]['labels2'][i]))\n",
    "            idx +=1\n",
    "    else:\n",
    "        for i, vid in enumerate(new_data_dic[par]['data']):\n",
    "            test_loader2.append((np.array(vid), new_data_dic[par]['labels2'][i]))\n",
    "            idx +=1\n",
    "print(len(data_loader2))\n",
    "print(data_loader2[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CNN(\n",
      "  (conv1): Sequential(\n",
      "    (0): Conv1d(1, 10, kernel_size=(3,), stride=(2,), padding=(1,))\n",
      "    (1): ReLU()\n",
      "    (2): MaxPool2d(kernel_size=(2, 1), stride=(2, 1), padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (conv1_2d): Sequential(\n",
      "    (0): Conv2d(1, 10, kernel_size=(3, 3), stride=(2, 2), padding=(2, 2))\n",
      "    (1): ReLU()\n",
      "    (2): MaxPool2d(kernel_size=(2, 1), stride=(2, 1), padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (conv2): Sequential(\n",
      "    (0): Conv1d(10, 20, kernel_size=(3,), stride=(2,), padding=(1,))\n",
      "    (1): ReLU()\n",
      "  )\n",
      "  (conv2_2d): Sequential(\n",
      "    (0): Conv2d(10, 20, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (1): ReLU()\n",
      "    (2): MaxPool2d(kernel_size=(2, 1), stride=(2, 1), padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (out): Linear(in_features=4160, out_features=2, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# this has all the main classes\n",
    "import torch.nn as nn\n",
    "# a neural network is a class derived from the core \"Module\" class in PyTorch\n",
    "class CNN(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(CNN, self).__init__()\n",
    "\n",
    "        self.conv1 = nn.Sequential(\n",
    "            nn.Conv1d(\n",
    "                in_channels=1,     \n",
    "                out_channels=10,   \n",
    "                kernel_size=3,     \n",
    "                stride=2,          \n",
    "                padding=1,         \n",
    "            ),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=(2,1)), \n",
    "        )\n",
    "        self.conv1_2d = nn.Sequential(  \n",
    "            nn.Conv2d(\n",
    "                in_channels=1,        \n",
    "                out_channels=10,      \n",
    "                kernel_size=3,        \n",
    "                stride=2,             \n",
    "                padding=2,            \n",
    "            ),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=(2,1)),\n",
    "        )\n",
    "        \n",
    "        self.conv2 = nn.Sequential(  \n",
    "            nn.Conv1d(10, 20, 3, 2, 1),\n",
    "            nn.ReLU(),                 \n",
    "        )\n",
    "        self.conv2_2d = nn.Sequential(  \n",
    "            nn.Conv2d(10, 20, 3, 1, 1), \n",
    "            nn.ReLU(),                  \n",
    "            nn.MaxPool2d(kernel_size=(2,1)),\n",
    "        )\n",
    "#         nn.ReLU()\n",
    "        nn.Softmax()\n",
    "        nn.Dropout(0.5)\n",
    "#         self.out = nn.Softmax()\n",
    "        self.out = nn.Linear(4160, 2)   # fully connected layer, output 1 class\n",
    "#         self.out = nn.Softplus(4160,2)\n",
    "    def forward(self, x):\n",
    "        if (train2D):\n",
    "            x = self.conv1_2d(x)\n",
    "            x = self.conv2_2d(x)\n",
    "        else:\n",
    "            x = self.conv1(x)\n",
    "            x = self.conv2(x)\n",
    "#         x = x.view(x.size(0), -1)\n",
    "        x = x.view(-1, 4160)# flatten the output \n",
    "        output = self.out(x)\n",
    "        \n",
    "        return output, x    # return x for potential visualization\n",
    "\n",
    "# create an instance of the class\n",
    "cnn = CNN()\n",
    "# print the network architecture\n",
    "print(cnn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "criterion sotfmarginLoss, MSELoss, ...."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "# standard cross entropy loss for classification\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "\n",
    "\n",
    "# I tried this first with stochastic gradient descent, but it DOES NOT WORK!\n",
    "optimizer = optim.SGD(cnn.parameters(), lr=0.0001, momentum=0.9)\n",
    "\n",
    "# so I switched to a more advanced training scheme, which works like magic!\n",
    "# optimizer = optim.Adam(cnn.parameters(),lr=0.00001, amsgrad=True)\n",
    "\n",
    "# optimizer = torch.optim.RMSprop(cnn.parameters(), lr=learning_rate)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "40\n"
     ]
    }
   ],
   "source": [
    "trX = []\n",
    "trY = []\n",
    "teX = []\n",
    "teY = []\n",
    "for i in data_loader:\n",
    "    trX.append(i[0])\n",
    "    trY.append(i[1][:2])\n",
    "for i in test_loader:\n",
    "    teX.append(i[0])\n",
    "    teY.append(i[1][:2])\n",
    "print(len(teY))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "Finished Training\n"
     ]
    }
   ],
   "source": [
    "cnn.train()\n",
    "train2D = True\n",
    "# let's do 30 epochs\n",
    "for epoch in range(5):\n",
    "    print(epoch)\n",
    "    # keep track of the current loss\n",
    "    running_loss = 0.0\n",
    "    training_error = 0.0\n",
    "    # get the current minibatch\n",
    "    for i, data in enumerate(data_loader, 0):\n",
    "        # get the inputs\n",
    "        inputs, labels = data\n",
    "        inputs = torch.Tensor(np.array(inputs).reshape(-1, 1, 32, 101))\n",
    "        labels = torch.Tensor(np.array(labels[:2]))\n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "        with torch.set_grad_enabled(True):\n",
    "            # forward + backward + optimize\n",
    "            outputs = cnn(inputs)\n",
    "            loss = criterion(outputs[0].view(-1), labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        if i<=-1 and epoch<=-1:\n",
    "            print(i,outputs[0],labels,loss.item())\n",
    "        # training error\n",
    "        pred_labels = np.sign(outputs[0].data.numpy().squeeze())\n",
    "        \n",
    "        training_error += np.sum(labels.numpy()!=pred_labels)/len(pred_labels)\n",
    "\n",
    "        # print statistics\n",
    "        running_loss += loss.item()\n",
    "        \n",
    "        if i % np.ceil(len(data_loader)/10) == np.ceil(len(data_loader)/10)-1: \n",
    "\n",
    "            running_loss = 0.0\n",
    "            training_error = 0.0\n",
    "print('Finished Training')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error on testing set = 37.500%\n"
     ]
    }
   ],
   "source": [
    "total_error=0\n",
    "for inputs, labels in test_loader:\n",
    "    inputs = torch.Tensor(np.array(inputs).reshape(-1, 1, 32, 101))\n",
    "    labels = torch.Tensor(labels[:2])\n",
    "    # push it through the network\n",
    "    test_output, _ = cnn(inputs)\n",
    "    # get the predicted labels by max activation\n",
    "    #pred_labels = torch.max(test_output, 1)[1].data.numpy().squeeze()\n",
    "    pred_labels = np.sign(test_output.data.numpy().squeeze())\n",
    "    # get the error by comparing to the ground truth\n",
    "    for i, label in enumerate(pred_labels):\n",
    "        pred_labels[i] = max(0, label)\n",
    "    total_error += 100*np.sum(labels.numpy()!=pred_labels)/len(pred_labels)\n",
    "print('Error on testing set = {:.3f}%'.format(total_error/len(test_loader)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CNN(\n",
      "  (conv1): Sequential(\n",
      "    (0): Conv1d(1, 10, kernel_size=(3,), stride=(2,), padding=(1,))\n",
      "    (1): ReLU()\n",
      "    (2): MaxPool2d(kernel_size=(2, 1), stride=(2, 1), padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (conv1_2d): Sequential(\n",
      "    (0): Conv2d(1, 10, kernel_size=(3, 3), stride=(2, 2), padding=(2, 2))\n",
      "    (1): ReLU()\n",
      "    (2): MaxPool2d(kernel_size=(2, 1), stride=(2, 1), padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (conv2): Sequential(\n",
      "    (0): Conv1d(10, 20, kernel_size=(3,), stride=(2,), padding=(1,))\n",
      "    (1): ReLU()\n",
      "  )\n",
      "  (conv2_2d): Sequential(\n",
      "    (0): Conv2d(10, 20, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (1): ReLU()\n",
      "    (2): MaxPool2d(kernel_size=(2, 1), stride=(2, 1), padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (out): Linear(in_features=4160, out_features=3, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# this has all the main classes\n",
    "import torch.nn as nn\n",
    "# a neural network is a class derived from the core \"Module\" class in PyTorch\n",
    "class CNN(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(CNN, self).__init__()\n",
    "\n",
    "        self.conv1 = nn.Sequential(\n",
    "            nn.Conv1d(\n",
    "                in_channels=1,     \n",
    "                out_channels=10,   \n",
    "                kernel_size=3,     \n",
    "                stride=2,          \n",
    "                padding=1,         \n",
    "            ),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=(2,1)), \n",
    "        )\n",
    "        self.conv1_2d = nn.Sequential(  \n",
    "            nn.Conv2d(\n",
    "                in_channels=1,        \n",
    "                out_channels=10,      \n",
    "                kernel_size=3,        \n",
    "                stride=2,             \n",
    "                padding=2,            \n",
    "            ),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=(2,1)),\n",
    "        )\n",
    "        \n",
    "        self.conv2 = nn.Sequential(  \n",
    "            nn.Conv1d(10, 20, 3, 2, 1),\n",
    "            nn.ReLU(),                 \n",
    "        )\n",
    "        self.conv2_2d = nn.Sequential(  \n",
    "            nn.Conv2d(10, 20, 3, 1, 1), \n",
    "            nn.ReLU(),                  \n",
    "            nn.MaxPool2d(kernel_size=(2,1)),\n",
    "        )\n",
    "#         nn.ReLU()\n",
    "        nn.Softmax()\n",
    "        nn.Dropout(0.5)\n",
    "#         self.out = nn.Softmax()\n",
    "        self.out = nn.Linear(4160, 3)   # fully connected layer, output 1 class\n",
    "#         self.out = nn.Softplus(4160,2)\n",
    "    def forward(self, x):\n",
    "        if (train2D):\n",
    "            x = self.conv1_2d(x)\n",
    "            x = self.conv2_2d(x)\n",
    "        else:\n",
    "            x = self.conv1(x)\n",
    "            x = self.conv2(x)\n",
    "#         x = x.view(x.size(0), -1)\n",
    "        x = x.view(-1, 4160)# flatten the output \n",
    "        output = self.out(x)\n",
    "        \n",
    "        return output, x    # return x for potential visualization\n",
    "\n",
    "# create an instance of the class\n",
    "cnn = CNN()\n",
    "# print the network architecture\n",
    "print(cnn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "Finished Training\n"
     ]
    }
   ],
   "source": [
    "cnn.train()\n",
    "\n",
    "for epoch in range(5):\n",
    "    print(epoch)\n",
    "    # keep track of the current loss\n",
    "    running_loss = 0.0\n",
    "    training_error = 0.0\n",
    "    # get the current minibatch\n",
    "    for i, data in enumerate(data_loader2, 0):\n",
    "        # get the inputs\n",
    "        inputs, labels = data\n",
    "        inputs = torch.Tensor(np.array(inputs).reshape(-1, 1, 32, 101))\n",
    "        labels = torch.Tensor(np.array(labels[:3]))\n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "        with torch.set_grad_enabled(True):\n",
    "            # forward + backward + optimize\n",
    "            outputs = cnn(inputs)\n",
    "            \n",
    "            loss = criterion(outputs[0].view(-1), labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "        if i<=-1 and epoch<=-1:\n",
    "            print(i,outputs[0],labels,loss.item())\n",
    "        # training error\n",
    "        pred_labels = np.sign(outputs[0].data.numpy().squeeze())\n",
    "        training_error += np.sum(labels.numpy()!=pred_labels)/len(pred_labels)\n",
    "        # print statistics\n",
    "        running_loss += loss.item()\n",
    "        \n",
    "        if i % np.ceil(len(data_loader2)/10) == np.ceil(len(data_loader2)/10)-1: \n",
    "\n",
    "            running_loss = 0.0\n",
    "            training_error = 0.0\n",
    "\n",
    "print('Finished Training')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3 class - test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error on testing set = 33.333%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\download\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:13: DeprecationWarning: elementwise != comparison failed; this will raise an error in the future.\n",
      "  del sys.path[0]\n"
     ]
    }
   ],
   "source": [
    "total_error=0\n",
    "for inputs, labels in test_loader2:\n",
    "    inputs = torch.Tensor(np.array(inputs).reshape(-1, 1, 32, 101))\n",
    "    labels = torch.Tensor(labels)\n",
    "    # push it through the network\n",
    "    test_output, _ = cnn(inputs)\n",
    "    # get the predicted labels by max activation\n",
    "    #pred_labels = torch.max(test_output, 1)[1].data.numpy().squeeze()\n",
    "    pred_labels = np.sign(test_output.data.numpy().squeeze())\n",
    "    for i, label in enumerate(pred_labels):\n",
    "        pred_labels[i] = max(0, label)\n",
    "    # get the error by comparing to the ground truth\n",
    "    total_error += 100*np.sum(labels.numpy()!=pred_labels)/len(pred_labels)\n",
    "print('Error on testing set = {:.3f}%'.format(total_error/len(test_loader)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
